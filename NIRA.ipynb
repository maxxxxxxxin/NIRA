{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\dask\\dataframe\\utils.py:369: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "D:\\Anaconda\\lib\\site-packages\\dask\\dataframe\\utils.py:369: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "D:\\Anaconda\\lib\\site-packages\\dask\\dataframe\\utils.py:369: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pybedtools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1f9647448dfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpybedtools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgffutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pybedtools'"
     ]
    }
   ],
   "source": [
    "from pyscenic.rnkdb import FeatherRankingDatabase as RankingDatabase\n",
    "from pyscenic.utils import modules_from_adjacencies\n",
    "from pyscenic.prune import prune2df, df2regulons\n",
    "from dask.diagnostics import ProgressBar\n",
    "from arboreto.utils import load_tf_names\n",
    "from arboreto.algo import grnboost2\n",
    "from pyscenic.aucell import aucell\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import gffutils\n",
    "import shutil\n",
    "import os, glob\n",
    "import ctxcore\n",
    "import pickle\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import essential files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data folder\n",
    "DATA_FOLDER=\"GSM4878210_L+H_E1_QC\"###your folder\n",
    "# GRNBoost input\n",
    "SC_EXP_FNAME = os.path.join(DATA_FOLDER, \"GSE115968.csv\")#inpput gene expression matrix\n",
    "MM_TFS_FNAME = os.path.join(DATA_FOLDER, 'mm_tfs.txt')#tf list\n",
    "\n",
    "#NIRA INPUT\n",
    "TSS_ANNOTATION = os.path.join(DATA_FOLDER, \"mm10TSS(/hg38TSS).csv\")#prebuilt database which can be downloaded from figshare'\n",
    "Hits = os.path.join(DATA_FOLDER, hitsFolder)#prebuilt database which can be downloaded from figshare' which instore all the occurance for each motifs\n",
    "ATAC_PEAK = os.path.join(DATA_FOLDER, \"ATACmm10.bed\")#input ATAC peaks\n",
    "#FILE SAVED\n",
    "MODULES_FNAME = os.path.join(DATA_FOLDER, \"modules.p\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grnboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_names = load_tf_names(MM_TFS_FNAME)\n",
    "db_fnames = glob.glob(DATABASES_GLOB)\n",
    "def name(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "dbs = [RankingDatabase(fname=fname, name=name(fname)) for fname in db_fnames]\n",
    "tic = time.time()\n",
    "ex_matrix = pd.read_csv(SC_EXP_FNAME, sep=',', header=0, index_col=0)#cell in col; gene in row\n",
    "#IF NEEDED:ex_matrix = ex_matrix.T\n",
    "adjancencies = grnboost2(expression_data=ex_matrix, tf_names=tf_names, verbose=True)\n",
    "modules = list(modules_from_adjacencies(adjancencies,ex_matrix,rho_mask_dropouts=True))\n",
    "with open(MODULES_FNAME, 'wb') as f:\n",
    "    pickle.dump(modules, f)\n",
    "tok = time.time() \n",
    "print(tok-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(modules)):\n",
    "    if modules[i].name == modules[0].name:\n",
    "        print(i)\n",
    "        break\n",
    "modules = modules[0:i-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------      NIRA functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIRAregulon(modules,TSSD,ATAC):\n",
    "\n",
    "######center the atac\n",
    "    ATAC = np.mat(ATAC)\n",
    "    ATAC[:,1] = (ATAC[:,1]+ATAC[:,2])/2-200\n",
    "    ATAC[:,2] = (ATAC[:,1]+ATAC[:,2])/2+200\n",
    "    ATAC=pd.DataFrame(ATAC)\n",
    "    ATAC[[1]]=ATAC[[1]].astype(int)\n",
    "    ATAC[[2]]=ATAC[[2]].astype(int)\n",
    "    ATACbed = BedTool.from_dataframe(ATAC)\n",
    "\n",
    "    mylist = []\n",
    "    dict_reg={}\n",
    "    for m in modules:\n",
    "        TFname = m.name.replace(\"Regulon for \",\"\").upper()\n",
    "        tss_all = TSSD[TSSD['external_gene_name'].isin(m.genes)]\n",
    "        tss_d = tss_all[[\"chromosome_name\",\"TSS-D\",\"TSS+D\",\"external_gene_name\",\"transcription_start_site\"]]\n",
    "        if os.path.exists(fileset+'//'+TFname+\".bed\") == True:\n",
    "            #calculate the backgroud based on the data population\n",
    "            hits = pd.read_table(\"hits//\"+TFname+\".bed\")\n",
    "            hits = BedTool.from_dataframe(hits)\n",
    "            hitsperpeak = ATACbed.intersect(hits,wa=True,wb=True)\n",
    "            hitsperpeak = hitsperpeak.to_dataframe(header=None)\n",
    "            mathits =np.mat(hitsperpeak)\n",
    "            #normalization\n",
    "            mathits[:,6]=(mathits[:,6]-mathits[:,6].min())/(mathits[:,6].max()-mathits[:,6].min())\n",
    "            hitsperpeak = pd.DataFrame(mathits)\n",
    "            hitsperpeak = BedTool.from_dataframe(hitsperpeak)\n",
    "            hitsperpeak = hitsperpeak.sort()\n",
    "            hitsperpeakbed = hitsperpeak.merge(c=\"7\", o=\"sum\")\n",
    "            hitsperpeak = hitsperpeakbed.to_dataframe(header=None)\n",
    "            #calculate the population\n",
    "            mean = np.mean(hitsperpeak[[\"name\"]])\n",
    "            var = np.std(hitsperpeak[[\"name\"]])\n",
    "            #calculate the confidence interval\n",
    "            cinter = stats.norm.interval(0.99, loc=mean, scale=var/math.sqrt(1000))\n",
    "\n",
    "            #for given regulons:\n",
    "            tss_all = TSSD[TSSD['external_gene_name'].isin(m.genes)]\n",
    "            tss_d = tss_all[[\"chromosome_name\",\"TSS-D\",\"TSS+D\",\"external_gene_name\",\"transcription_start_site\"]]\n",
    "            x = BedTool.from_dataframe(tss_d)\n",
    "            peakswithin = x.intersect(hitsperpeakbed,wa=True,wb=True)\n",
    "            hits0perpeak = peakswithin.to_dataframe(header=None)\n",
    "            y = hits0perpeak[[\"strand\",\"thickStart\",\"thickEnd\",\"itemRgb\"]]\n",
    "            y = BedTool.from_dataframe(y)\n",
    "            y = y.sort()\n",
    "            y = y.merge(c=\"4\", o=\"mean\")\n",
    "            y = y.to_dataframe(header=None)\n",
    "            nhits0 = np.mean(y[[\"name\"]])[0]\n",
    "            if nhits0>=cinter[1]:\n",
    "                peaksperhits0within = peakswithin.sort()\n",
    "                ssdd = peaksperhits0within.merge(c=\"4,5,9\", o=\"distinct,mean,sum\")\n",
    "                Mat = calculate(ssdd)\n",
    "                Mat.columns = [\"GENE\",\"SCORE\"]\n",
    "                #m_1 = file.shape[0]\n",
    "                #file_1 = Mat.loc[file[\"SCORE\"]>#####]\n",
    "                #m_2 = file_1.shape[0]\n",
    "                #if m_2 >20:\n",
    "                innerdict = Mat.set_index('GENE').T.to_dict('list')\n",
    "                #regulons = Mat[\"GENE\"].values.to_list()\n",
    "                scores = Mat['SCORE'].values.to_list()\n",
    "                mylist.append(scores)\n",
    "                dict_reg[TFname]=innerdict\n",
    "    \n",
    "    myonelist =[i for p in mylist for i in p]\n",
    "    thres = threshentroy(myonelist)[0]\n",
    "    filter_reg={}\n",
    "    for i in len(dict_reg):\n",
    "        REGname = dict_reg[i]\n",
    "        filter_Reg ={k for k,v in dict_reg[REGname].items() if v[0]>=thres}\n",
    "        #filter_Reg = dict(filter(lambda item:item>=thres,regulons.item()))\n",
    "        filter_reg[REGname]=filter_Reg\n",
    "        \n",
    "    return filter_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "side functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(ssdd):\n",
    "    file = ssdd.to_dataframe(header=None,names=[\"Chr\",\"start\",\"end\",\"genename\",\"genetss\",\"score\"])\n",
    "    tt = file[\"genename\"].str.split(\",\",expand=True)\n",
    "    if tt.shape[1] == 1:\n",
    "        x = file [[\"start\"]]\n",
    "        x = np.mat(x)\n",
    "        y = file[[\"end\"]]\n",
    "        y = np.mat(y)\n",
    "        z=file[[\"genetss\"]]\n",
    "        z=np.mat(z)\n",
    "        d = abs(z-(x+y)/2)\n",
    "        d = d.astype(float)\n",
    "        d = np.exp(-d/5000)\n",
    "        s=file[[\"score\"]]\n",
    "        s=np.mat(s)\n",
    "        bc_ = d*s.T\n",
    "        diagonal = np.diag_indices(len(d))\n",
    "        bc_ = bc_[diagonal]\n",
    "        gene=file[[\"genename\"]]\n",
    "        gene=np.mat(gene)\n",
    "        bc = np.append(gene,bc_.T,axis=1)\n",
    "        bc= pd.DataFrame(bc)\n",
    "        bc.columns=[\"gene\",\"score\"]\n",
    "        \n",
    "        l = bc.groupby('gene').agg({'score':np.sum})\n",
    "        id = l.index \n",
    "        id = id.tolist()\n",
    "        SVmatrix = l[[\"score\"]]\n",
    "        SVmatrix=SVmatrix.values.tolist()\n",
    "        SVmatrix =np.mat(SVmatrix)\n",
    "        t = pd.DataFrame(m.weights)\n",
    "        t.index = m.genes\n",
    "        t.columns = [\"IM\"]\n",
    "        IM  = np.mat(t.loc[id][[\"IM\"]])\n",
    "        Wmatrix = IM*SVmatrix.T\n",
    "        diagonal = np.diag_indices(len(IM))\n",
    "        Wmatrix = Wmatrix[diagonal]\n",
    "        Wmatrix = pd.DataFrame(Wmatrix.T)\n",
    "        id = pd.DataFrame(id)\n",
    "        Wdf = pd.concat([id,Wmatrix],axis=1)\n",
    "    else: \n",
    "       \n",
    "        r = tt[tt[1].notnull()== True].index\n",
    "        v = pd.DataFrame(columns = [\"Chr\",\"start\",\"end\",\"genename\",\"genetss\",\"score\"])\n",
    "        for j in range(len(r)):\n",
    "            s = r[j] \n",
    "            n = len(file.loc[s][\"genename\"].split(\",\"))\n",
    "            for i in range(n):\n",
    "                xx = file.loc[s]\n",
    "                xx[\"genename\"] = file.loc[s][\"genename\"].split(\",\")[i]\n",
    "                v.loc[2*j+i]=xx\n",
    "        file = file.append(v, ignore_index=True)\n",
    "        file = file.drop(r)\n",
    "\n",
    "        x = file [[\"start\"]]\n",
    "        x = np.mat(x)\n",
    "        y = file[[\"end\"]]\n",
    "        y = np.mat(y)\n",
    "        z=file[[\"genetss\"]]\n",
    "        z=np.mat(z)\n",
    "        d = abs(z-(x+y)/2)\n",
    "        d = d.astype(float)\n",
    "        d = np.exp(-d/5000)\n",
    "        s=file[[\"score\"]]\n",
    "        s=np.mat(s)\n",
    "        bc_ = d*s.T\n",
    "        diagonal = np.diag_indices(len(d))\n",
    "        bc_ = bc_[diagonal]\n",
    "        gene=file[[\"genename\"]]\n",
    "        gene=np.mat(gene)\n",
    "        bc = np.append(gene,bc_.T,axis=1)\n",
    "        bc= pd.DataFrame(bc)\n",
    "        bc.columns=[\"gene\",\"score\"]\n",
    "        l = bc.groupby('gene').agg({'score':np.sum})\n",
    "        id = l.index \n",
    "        id = id.tolist()\n",
    "        SVmatrix = l[[\"score\"]]\n",
    "        SVmatrix=SVmatrix.values.tolist()\n",
    "        SVmatrix =np.mat(SVmatrix)\n",
    "        t = pd.DataFrame(m.weights)\n",
    "        t.index = m.genes\n",
    "        t.columns = [\"IM\"]\n",
    "        IM  = np.mat(t.loc[id][[\"IM\"]])\n",
    "        Wmatrix = IM*SVmatrix.T\n",
    "        diagonal = np.diag_indices(len(IM))\n",
    "        Wmatrix = Wmatrix[diagonal]\n",
    "        Wmatrix = pd.DataFrame(Wmatrix.T)\n",
    "        id = pd.DataFrame(id)\n",
    "        Wdf = pd.concat([id,Wmatrix],axis=1)\n",
    "    return Wdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshentroy(x):\n",
    "    n0 = len(x)\n",
    "    maxv = max(x)##### modification could be used\n",
    "    ######## probhist\n",
    "    counts0 = np.histogram(x,bins=int(maxv))\n",
    "    newcounts =list(filter(lambda x:x>10, counts0[0]))\n",
    "    counts = np.mat(newcounts)\n",
    "    counts = counts/n0\n",
    "    counts = counts.tolist()\n",
    "    counts = counts[0]\n",
    "   \n",
    "    n = len(counts)\n",
    "    #####sumprob for each cut\n",
    "    eee = np.zeros(n)\n",
    "    \n",
    "    for k in range(n):\n",
    "        amat = np.zeros(n)\n",
    "        asum = np.sum(counts[:k+1])\n",
    "        for j in range(k+1):\n",
    "            if counts[j]==0 or asum ==0:\n",
    "                amat[j]=0\n",
    "            else:\n",
    "                amat[j] = -(counts[j]/asum)*(np.log10((counts[j]/asum)))\n",
    "        for i in range(k+1,n):\n",
    "            if counts[i]==0 or asum==1:\n",
    "                amat[i]=0\n",
    "            else:\n",
    "                amat[i] =-(counts[i]/(1-asum))*(np.log10(counts[i]/(1-asum)))\n",
    "        \n",
    "        f1= np.sum(amat[:k+1])\n",
    "        f2= np.sum(amat[k+1:])\n",
    "        eee[k]=f1+f2\n",
    "\n",
    "    threshold = np.where(eee==np.max(eee))\n",
    "    thres = threshold[0][0]\n",
    "    return counts0[1][thres],threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS_ANNOTATION = pd.read_csv(SC_EXP_FNAME, sep=',', header=0)\n",
    "ATAC_PEAK = pd.read_table(ATAC_PEAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulons = NIRAregulon(modules,TSS_ANNOTATION,ATAC_PEAK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------- AUCELL for each regulon input and BOOTSTRAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank the expression data for each cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyscenic.genesig import GeneSignature\n",
    "from pyscenic.aucell import create_rankings, enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genesets = ex_matrix.T.index\n",
    "nreg =len(filter_reg)\n",
    "ngene = len(genesets)\n",
    "m,n=np.shape(ex_matrix)\n",
    "nTest=10000\n",
    "#original AUC\n",
    "M =np.zeros([0,0])\n",
    "M=pd.DataFrame(M)\n",
    "#BOOTSTRAP AUC\n",
    "N =np.zeros([0,0])\n",
    "N=pd.DataFrame(N)\n",
    "#binary AUC\n",
    "R = np.zeros(shape=(m,nTest+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnk_mtx = create_rankings(ex_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in filter_reg:\n",
    "    reg = filter_reg[i]\n",
    "    s = len(reg)\n",
    "    gs0 = GeneSignature(i,[line.strip() for idx, line in enumerate(reg)])\n",
    "    aucs0 = enrichment(rnk_mtx, gs)\n",
    "    vv = np.mat(aucs0[[\"AUC\"]])\n",
    "    R[:,0] = vv.T\n",
    "    M[[i]] = pd.DataFrame(vv)\n",
    "    for j in range(0,nTest):\n",
    "        rs = np.random.choice(genesets,s,False)\n",
    "        gs = GeneSignature(i,[line.strip() for idx, line in enumerate(rs)])\n",
    "        aucs = enrichment(rnk_mtx, gs)\n",
    "        R[:,j+1] = np.mat(aucs[[\"AUC\"]]).T\n",
    "\n",
    "    Rm = R[:,1:].T- R[:,0].T\n",
    "    Rm=Rm.T\n",
    "    Pm = pd.DataFrame(Rm)\n",
    "    pv = ((Pm >= 0).astype(int).sum(axis=1))/nTest\n",
    "    pv = pd.DataFrame(pv)\n",
    "    N[[i]]= pv\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
